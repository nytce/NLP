{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nytce/NLP/blob/main/CLUEbenchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791ih8m8EBC4"
      },
      "source": [
        "##数据预处理："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqLaRFEv6YD7"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU_AVCaNmugd"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhG9ZnIY7gVW"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler, RandomSampler\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "# import paddlenlp as ppnlp\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
        "# from transformers import LinearDecayWithWarmup\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKyThT1lsz1W"
      },
      "outputs": [],
      "source": [
        "def read_json(input_file):\n",
        "    with open(input_file, \"r\") as f:\n",
        "        reader = f.readlines()\n",
        "        lines = []\n",
        "        for line in reader:\n",
        "            lines.append(json.loads(line.strip()))\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JZhz_Gyunmt"
      },
      "outputs": [],
      "source": [
        "# 自定义数据集类\n",
        "class CLUE_Dataset(Dataset):\n",
        "    def __init__(self, input_data):\n",
        "      # # 使用 read_json 函数加载数据\n",
        "      # self.data = read_json(input_file)\n",
        "      if isinstance(input_data, list):\n",
        "        # 如果传入的是列表，则直接将列表存储为数据集\n",
        "        self.data = input_data\n",
        "      elif isinstance(input_data, (str, Path)):\n",
        "        # 如果传入的是文件路径，则读取数据文件\n",
        "        self.data = read_json(input_data)\n",
        "      else:\n",
        "        raise ValueError(\"Unsupported input_data type. Expecting list, str, or Path.\")\n",
        "\n",
        "    def __len__(self):\n",
        "      # 返回数据集大小\n",
        "      return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # 返回指定索引的数据\n",
        "      return self.data[index]\n",
        "\n",
        "    def map(self, func):\n",
        "      # 对数据集中的每个样本应用自定义的转换函数\n",
        "      self.data = list(map(func, self.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKQnB0AkYJra",
        "outputId": "76a200c0-2628-4b91-b3f5-e22affad7889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 导入Google drive库\n",
        "from google.colab import drive\n",
        "\n",
        "# 挂载Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 数据集所在文件夹路径\n",
        "dataset_folder = '/content/drive/My Drive/CLUEdataset/afqmc_public'\n",
        "\n",
        "# 训练集和验证集文件路径\n",
        "train_file_path = os.path.join(dataset_folder, 'train.json')\n",
        "dev_file_path = os.path.join(dataset_folder, 'dev.json')\n",
        "test_file_path = os.path.join(dataset_folder, 'test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hs8dzkbtCe1"
      },
      "outputs": [],
      "source": [
        "train_ds = CLUE_Dataset(train_file_path)\n",
        "dev_ds = CLUE_Dataset(dev_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNkZNdMeqftP",
        "outputId": "d1784374-9202-41ab-e8d3-b72d233a0f37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence1': '蚂蚁借呗等额还款可以换成先息后本吗', 'sentence2': '借呗有先息到期还本吗', 'label': '0'}\n",
            "{'sentence1': '蚂蚁花呗说我违约一次', 'sentence2': '蚂蚁花呗违约行为是什么', 'label': '0'}\n",
            "{'sentence1': '帮我看一下本月花呗账单有没有结清', 'sentence2': '下月花呗账单', 'label': '0'}\n",
            "{'sentence1': '蚂蚁借呗多长时间综合评估一次', 'sentence2': '借呗得评估多久', 'label': '0'}\n",
            "{'sentence1': '我的花呗账单是***，还款怎么是***', 'sentence2': '我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元', 'label': '1'}\n"
          ]
        }
      ],
      "source": [
        "# 输出训练集的前 2 条样本\n",
        "for idx, example in enumerate(train_ds):\n",
        "    if idx <= 4:\n",
        "        print(example)\n",
        "        #print(example[\"sentence1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxrejVxBww8S"
      },
      "outputs": [],
      "source": [
        "class BERT_RNN(nn.Module):\n",
        "    def __init__(self, bert_model_name, hidden_size, output_size):\n",
        "        super(BERT_RNN, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.rnn = nn.RNN(self.bert.config.hidden_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_output = self.bert(input_ids, attention_mask)[0]\n",
        "        rnn_output, _ = self.rnn(bert_output)\n",
        "        output = self.linear(rnn_output[:, -1, :])  # 使用 RNN 输出的最后一个时间步作为输入\n",
        "        return output\n",
        "\n",
        "my_model = BERT_RNN('bert-base-uncased', hidden_size=256, output_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSQ5IjZhbB7Z"
      },
      "outputs": [],
      "source": [
        "# 使用 ERNIE-Gram 预训练模型\n",
        "# pretrained_model = ppnlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
        "# tokenizer = ppnlp.transformers.ErnieGramTokenizer.from_pretrained('ernie-gram-zh')\n",
        "\n",
        "# 使用 ERNIE 预训练模型\n",
        "# ernie-1.0\n",
        "#pretrained_model = ppnlp.transformers.ErnieModel.from_pretrained('ernie-1.0'))\n",
        "#tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained('ernie-1.0')\n",
        "\n",
        "# ernie-tiny\n",
        "# pretrained_model = ppnlp.transformers.ErnieModel.from_pretrained('ernie-tiny'))\n",
        "# tokenizer = ppnlp.transformers.ErnieTinyTokenizer.from_pretrained('ernie-tiny')\n",
        "\n",
        "\n",
        "# 使用 BERT 预训练模型\n",
        "# bert-base-chinese\n",
        "# pretrained_model = ppnlp.transformers.BertModel.from_pretrained('bert-base-chinese')\n",
        "# tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "\n",
        "# bert-wwm-chinese\n",
        "# pretrained_model = ppnlp.transformers.BertModel.from_pretrained('bert-wwm-chinese')\n",
        "# tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained('bert-wwm-chinese')\n",
        "\n",
        "# bert-wwm-ext-chinese\n",
        "# pretrained_model = ppnlp.transformers.BertModel.from_pretrained('bert-wwm-ext-chinese')\n",
        "# tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained('bert-wwm-ext-chinese')\n",
        "\n",
        "\n",
        "# 使用 RoBERTa 预训练模型\n",
        "# roberta-wwm-ext\n",
        "# pretrained_model = ppnlp.transformers.RobertaModel.from_pretrained('roberta-wwm-ext')\n",
        "# tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained('roberta-wwm-ext')\n",
        "\n",
        "# roberta-wwm-ext\n",
        "# pretrained_model = ppnlp.transformers.RobertaModel.from_pretrained('roberta-wwm-ext-large')\n",
        "# tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained('roberta-wwm-ext-large')\n",
        "\n",
        "# 自己的模型\n",
        "predicted_model = BERT_RNN(bert_model_name='bert-base-chinese', hidden_size=256, output_size=2)\n",
        "# 初始化分词器\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0pI7WY13zOO"
      },
      "outputs": [],
      "source": [
        "# 转换成id的函数\n",
        "def convert_example(example, tokenizer):\n",
        "    encoded_inputs = tokenizer(text=example[\"sentence1\"],text_pair=example[\"sentence2\"], max_length=128, padding='max_length')\n",
        "    return tuple([np.array(x, dtype=\"int64\") for x in [\n",
        "            encoded_inputs[\"input_ids\"], encoded_inputs[\"token_type_ids\"], [example[\"label\"]]]])\n",
        "\n",
        "# 加载BERT的分词器\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
        "\n",
        "# 把训练集合转换成id\n",
        "# train_ds = train_ds.map(partial(convert_example, tokenizer=tokenizer))\n",
        "# # 把验证集合转换成id\n",
        "# dev_ds = dev_ds.map(partial(convert_example, tokenizer=tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fr5KhKA7cdh"
      },
      "outputs": [],
      "source": [
        "# 逐一转换\n",
        "def process(Dataset):\n",
        "  processed_data = []\n",
        "  for example in Dataset:\n",
        "    try:\n",
        "      # 调用 convert_example 函数处理示例\n",
        "      processed_example = convert_example(example, tokenizer)\n",
        "\n",
        "      # 将处理后的结果添加到列表中\n",
        "      processed_data.append(processed_example)\n",
        "    except Exception as e:\n",
        "      # 捕获异常并处理\n",
        "      print(f\"Error processing example: {e}\")\n",
        "      # 可以选择跳过当前示例，继续处理下一个示例\n",
        "      continue\n",
        "  return CLUE_Dataset(processed_data)\n",
        "\n",
        "# 保存为数据集\n",
        "train_tokenized_ds = process(train_ds)\n",
        "dev_tokenized_ds = process(dev_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_tokenized_ds))\n",
        "# 这里打印的是tokenzier转换过后的数据\n",
        "for idx, example in enumerate(train_tokenized_ds):\n",
        "    if idx <= 4:\n",
        "        print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ1QNyYTC9NU",
        "outputId": "d03fb012-bfe7-4829-96e8-63ca3ee33a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.CLUE_Dataset'>\n",
            "(array([ 101, 6010, 6009,  955, 1446, 5023, 7583, 6820, 3621, 1377,  809,\n",
            "       2940, 2768, 1044, 2622, 1400, 3315, 1408,  102,  955, 1446, 3300,\n",
            "       1044, 2622, 1168, 3309, 6820, 3315, 1408,  102,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0]))\n",
            "(array([ 101, 6010, 6009, 5709, 1446, 6432, 2769, 6824, 5276,  671, 3613,\n",
            "        102, 6010, 6009, 5709, 1446, 6824, 5276, 6121,  711, 3221,  784,\n",
            "        720,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0]))\n",
            "(array([ 101, 2376, 2769, 4692,  671,  678, 3315, 3299, 5709, 1446, 6572,\n",
            "       1296, 3300, 3766, 3300, 5310, 3926,  102,  678, 3299, 5709, 1446,\n",
            "       6572, 1296,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0]))\n",
            "(array([ 101, 6010, 6009,  955, 1446, 1914, 7270, 3198, 7313, 5341, 1394,\n",
            "       6397,  844,  671, 3613,  102,  955, 1446, 2533, 6397,  844, 1914,\n",
            "        719,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0]))\n",
            "(array([ 101, 2769, 4638, 5709, 1446, 6572, 1296, 3221,  115,  115,  115,\n",
            "       8024, 6820, 3621, 2582,  720, 3221,  115,  115,  115,  102, 2769,\n",
            "       4638, 5709, 1446, 8024, 3299, 5310, 1139, 3341, 6432, 6375, 2769,\n",
            "       6820,  115,  115,  115, 1039, 8024, 2769, 5632, 2346, 5050,  749,\n",
            "        671,  678, 6422, 5301, 1399, 1296, 2769, 2418, 6421, 6820,  115,\n",
            "        115,  115, 1039,  102,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn_J59d_CQc1"
      },
      "outputs": [],
      "source": [
        "# 构建训练集合的dataloader\n",
        "train_batch_size=32\n",
        "dev_batch_size=32\n",
        "# train_batch_sampler = DistributedSampler(dataset=train_ds, batch_size=train_batch_size, shuffle=True)\n",
        "# train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, return_list=True)\n",
        "\n",
        "# train_smapler = DistributedSampler(dataset=train_ds)\n",
        "# train_data_loader = DataLoader(dataset=train_ds, batch_size=train_batch_size, shuffle=True, sampler=train_sampler)\n",
        "train_data_loader = DataLoader(dataset=train_tokenized_ds, batch_size=train_batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "# 针对验证集数据加载，我们使用单卡进行评估，所以采用 paddle.io.BatchSampler 即可\n",
        "# 定义验证集的dataloader\n",
        "# dev_batch_sampler = DistributedSampler(dev_ds, batch_size=dev_batch_size, shuffle=False)\n",
        "\n",
        "# dev_data_loader = DataLoader(\n",
        "#         dataset=dev_ds,\n",
        "#         batch_sampler=dev_batch_sampler,\n",
        "#         return_list=True)\n",
        "# dev_smapler = RandomSampler(dataset=dev_ds)\n",
        "# dev_data_loader = DataLoader(dataset=dev_ds, batch_size=dev_batch_size, shuffle=True, sampler=dev_sampler)\n",
        "dev_data_loader = DataLoader(dataset=dev_tokenized_ds, batch_size=dev_batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDTNRsU8CKkx"
      },
      "source": [
        "##模型构建：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92nyHYSFCRD2"
      },
      "outputs": [],
      "source": [
        "class BERT_RNN(nn.Module):\n",
        "    def __init__(self, bert_model_name, hidden_size, output_size):\n",
        "        super(BERT_RNN, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.rnn = nn.RNN(self.bert.config.hidden_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):#token_type_ids=token_type_ids,\n",
        "        bert_output = self.bert(input_ids, attention_mask)[0]  #token_type_ids=token_type_ids,\n",
        "        rnn_output, _ = self.rnn(bert_output)\n",
        "        output = self.linear(rnn_output[:, -1, :])  # 使用 RNN 输出的最后一个时间步作为输入\n",
        "\n",
        "        print(bert_output.shape)\n",
        "        print(rnn_output.shape)\n",
        "        print(output.shape)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rJ7dkWoNrNb"
      },
      "outputs": [],
      "source": [
        "model = BERT_RNN('bert-base-uncased', hidden_size=256, output_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEd9zqoyEZMu"
      },
      "source": [
        "##训练配置："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp6iQZDnEgHK"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "train_data_loader = train_data_loader\n",
        "num_training_steps = len(train_data_loader) * epochs\n",
        "\n",
        "# 定义优化器和初始学习率\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "# 定义总的训练步数和预热步数\n",
        "total_steps = len(train_data_loader) * epochs\n",
        "warmup_steps = 0.1 * total_steps  # 预热步数为总步数的 10%\n",
        "\n",
        "# 定义学习率调度器函数\n",
        "lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda step: min((step + 1) / warmup_steps, 1.0))\n",
        "\n",
        "\n",
        "# 交叉熵损失\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # 评估的时候采用准确率指标\n",
        "# metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTkVoHxfEcAe"
      },
      "source": [
        "##模型训练："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNhcn6m3Efoa"
      },
      "outputs": [],
      "source": [
        "# 因为训练过程中同时要在验证集进行模型评估，因此我们先定义评估函数\n",
        "# @torch.no_grad()\n",
        "# def evaluate(model, criterion, metric, data_loader, phase=\"dev\"):\n",
        "#     model.eval()\n",
        "#     metric.reset()\n",
        "#     losses = []\n",
        "#     true_labels = []\n",
        "#     predicted_labels = []\n",
        "\n",
        "#     for batch in data_loader:\n",
        "#         input_ids, token_type_ids, labels = batch\n",
        "#         outputs = model(input_ids, token_type_ids)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         losses.append(loss.item())\n",
        "\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         true_labels.extend(labels.tolist())\n",
        "#         predicted_labels.extend(predicted.tolist())\n",
        "\n",
        "#     true_labels = np.array(true_labels)\n",
        "#     predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "#     accuracy = (predicted_labels == true_labels).mean()\n",
        "\n",
        "#     # 计算召回率、精确率和 F1 分数\n",
        "#     tp = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
        "#     fp = np.sum((predicted_labels == 1) & (true_labels == 0))\n",
        "#     fn = np.sum((predicted_labels == 0) & (true_labels == 1))\n",
        "\n",
        "#     recall = tp / (tp + fn)\n",
        "#     precision = tp / (tp + fp)\n",
        "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "#     print(f\"eval {phase} loss: {np.mean(losses):.5f}, accu: {accuracy:.5f}, recall: {recall:.5f}, precision: {precision:.5f}, f1-score: {f1_score:.5f}\")\n",
        "\n",
        "#     model.train()\n",
        "#     return np.mean(losses), accuracy\n",
        "\n",
        "# from datasets import load_metric\n",
        "\n",
        "# accuracy_metric = load_metric(\"accuracy\")\n",
        "# recall_metric = load_metric(\"recall\")\n",
        "# f1_metric = load_metric(\"f1\")\n",
        "\n",
        "# def evaluate(model, criterion, data_loader, phase=\"dev\"):\n",
        "#     model.eval()\n",
        "#     losses = []\n",
        "#     true_labels = []\n",
        "#     predicted_labels = []\n",
        "\n",
        "#     for batch in data_loader:\n",
        "#         input_ids, token_type_ids, labels = batch\n",
        "#         outputs = model(input_ids, token_type_ids)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         losses.append(loss.item())\n",
        "\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         true_labels.extend(labels.tolist())\n",
        "#         predicted_labels.extend(predicted.tolist())\n",
        "\n",
        "#     accuracy = accuracy_metric.compute(references=true_labels, predictions=predicted_labels)\n",
        "#     recall = recall_metric.compute(references=true_labels, predictions=predicted_labels)\n",
        "#     f1 = f1_metric.compute(references=true_labels, predictions=predicted_labels)\n",
        "\n",
        "#     print(f\"eval {phase} loss: {np.mean(losses):.5f}, accuracy: {accuracy:.5f}, recall: {recall:.5f}, f1-score: {f1:.5f}\")\n",
        "\n",
        "#     model.train()\n",
        "#     return np.mean(losses), accuracy, recall, f1\n",
        "def evaluate(model, criterion, data_loader, phase=\"dev\"):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            true_labels.extend(labels.tolist())\n",
        "            predicted_labels.extend(predicted.tolist())\n",
        "\n",
        "    accuracy = (torch.tensor(true_labels) == torch.tensor(predicted_labels)).float().mean().item()\n",
        "\n",
        "    # 计算召回率、精确率和 F1 分数\n",
        "    tp = sum((p == 1 and l == 1) for p, l in zip(predicted_labels, true_labels))\n",
        "    fp = sum((p == 1 and l == 0) for p, l in zip(predicted_labels, true_labels))\n",
        "    fn = sum((p == 0 and l == 1) for p, l in zip(predicted_labels, true_labels))\n",
        "\n",
        "    recall = tp / (tp + fn)\n",
        "    precision = tp / (tp + fp)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    print(f\"eval {phase} loss: {np.mean(losses):.5f}, accu: {accuracy:.5f}, recall: {recall:.5f}, precision: {precision:.5f}, f1-score: {f1_score:.5f}\")\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(losses), accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq2dishCRA3R"
      },
      "outputs": [],
      "source": [
        "# 接下来，开始正式训练模型，训练时间较长，可注释掉这部分\n",
        "def train(model, criterion, optimizer, scheduler, dev_data_loader, train_data_loader, epochs=3):\n",
        "    global_step = 0\n",
        "    tic_train = time.time()\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()  # 将模型切换到训练模式\n",
        "        for step, batch in enumerate(train_data_loader, start=1):\n",
        "            input_ids, token_type_ids, labels = batch\n",
        "            optimizer.zero_grad()  # 清除梯度\n",
        "            probs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask) #加一个退出函数\n",
        "            exit()\n",
        "            loss = criterion(probs, labels)\n",
        "            loss.backward()  # 反向传播\n",
        "            optimizer.step()  # 更新参数\n",
        "            scheduler.step()  # 更新学习率\n",
        "\n",
        "            # 更新训练指标\n",
        "            correct = (torch.argmax(probs, dim=1) == labels).sum().item()\n",
        "            total = labels.size(0)\n",
        "            accuracy = correct / total\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # 每间隔 100 step 输出训练指标\n",
        "            if global_step % 100 == 0:\n",
        "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
        "                      % (global_step, epoch, step, loss.item(), accuracy, 10 / (time.time() - tic_train)))\n",
        "                tic_train = time.time()\n",
        "\n",
        "            # 每间隔 500 step 在验证集上进行评估\n",
        "            if global_step % 500 == 0:\n",
        "                eval_loss, eval_acc = evaluate(model, criterion, dev_data_loader, \"dev\")\n",
        "                if best_accuracy < eval_acc:\n",
        "                    best_accuracy = eval_acc\n",
        "                    # # 保存模型\n",
        "                    # save_param_path = os.path.join(save_dir, 'model_best.pth')\n",
        "                    # torch.save(model.state_dict(), save_param_path)\n",
        "                    # 保存tokenizer\n",
        "                    # tokenizer.save_pretrained(save_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-zaHfmynJxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "d8d9c521-66ad-4ce0-ac03-d76b08f97b4a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BERT_RNN.forward() got an unexpected keyword argument 'token_type_ids'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-df324fbb6ef6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate(model, criterion, dev_data_loader, phase=\"dev\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-404a6ed1b50c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, scheduler, dev_data_loader, train_data_loader, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 清除梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#加一个退出函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BERT_RNN.forward() got an unexpected keyword argument 'token_type_ids'"
          ]
        }
      ],
      "source": [
        "# evaluate(model, criterion, dev_data_loader, phase=\"dev\")\n",
        "train(model, criterion, optimizer, lr_scheduler, dev_data_loader, train_data_loader, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqHo-y82EibG"
      },
      "source": [
        "##模型预测："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6cjtIjEEkk4"
      },
      "outputs": [],
      "source": [
        "# state_dict=paddle.load('checkpoint/model_best.pdparams')\n",
        "# model.load_dict(state_dict)\n",
        "\n",
        "# state_dict = torch.load('checkpoint/model_best.pth')\n",
        "# model.load_state_dict(state_dict)\n",
        "\n",
        "# 不保存模型就不用这一步，直接加载模型就行\n",
        "model = BERT_RNN('bert-base-uncased', hidden_size=256, output_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZZ2Ze0dRRDU"
      },
      "outputs": [],
      "source": [
        "test_ds = CLUE_Dataset(test_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC_3L_QKRaah"
      },
      "outputs": [],
      "source": [
        "def do_predict(model, example, tokenizer):\n",
        "    # 把文本转换成input_ids, token_type_ids\n",
        "    encoded_text = tokenizer(text=example[\"sentence1\"], text_pair=example[\"sentence2\"], max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "    # encoded_text = tokenizer(text=example[\"sentence1\"],text_pair=example[\"sentence2\"], max_length=512, padding='max_length')\n",
        "    # 把input_ids变成PyTorch tensor\n",
        "    input_ids = encoded_text['input_ids']\n",
        "    # 把token_type_ids变成PyTorch tensor\n",
        "    segment_ids = encoded_text['token_type_ids']\n",
        "\n",
        "    # 模型预测\n",
        "    with torch.no_grad():\n",
        "        pooled_output = model(input_ids, segment_ids)\n",
        "\n",
        "    # 取概率值最大的索引\n",
        "    _, out2 = torch.max(pooled_output, axis=1)\n",
        "\n",
        "    return out2.item()\n",
        "\n",
        "# 预测测试集\n",
        "# predict_label = []\n",
        "# for example in tqdm(test_ds):\n",
        "#     label_pred = do_predict(model, example, tokenizer)\n",
        "#     predict_label.append(label_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo_qjklo5ZEP"
      },
      "outputs": [],
      "source": [
        "# 打印测试集预测结果\n",
        "# print(predict_label[:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 预测验证集\n",
        "dev_predict_label = []\n",
        "for example in tqdm(dev_ds):\n",
        "    label_pred = do_predict(model, example, tokenizer)\n",
        "    dev_predict_label.append(label_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rSqBZ1-1oGW",
        "outputId": "f3247383-603f-4b27-9c7f-d49942fc747f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4316/4316 [1:17:07<00:00,  1.07s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_predict_label[3000:3100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "JmC26isiXOF5",
        "outputId": "345cd5f6-cbae-4908-ae4b-3825f9a4be64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dev_predict_label' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-241cc43fcda9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_predict_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dev_predict_label' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 假设真实标签存储在验证集数据集的 'labels' 字段中\n",
        "true_labels = [example[\"label\"] for example in dev_ds]\n",
        "\n",
        "# 计算准确率\n",
        "accuracy = accuracy_score(true_labels, dev_predict_label)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "4hy6HovX1ono",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1510bf-0b12-4147-e508-7fd32a98227c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqrMnEFeYFA"
      },
      "source": [
        "## 评价指标"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxb1PWuCgmmM"
      },
      "outputs": [],
      "source": [
        "# for batch in data_loader:\n",
        "#     input_ids, attention_mask, labels = batch\n",
        "#     outputs = model(input_ids, attention_mask)\n",
        "#     predicted_labels = torch.argmax(outputs, dim=1)\n",
        "#     acc = accuracy(predicted_labels, labels)\n",
        "\n",
        "# def accuracy(predictions, labels):\n",
        "#     _, predicted = torch.max(predictions, 1)\n",
        "#     correct = (predicted == labels).sum().item()\n",
        "#     total = labels.size(0)\n",
        "#     accuracy = correct / total\n",
        "#     return accuracy\n",
        "# outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "# predicted_labels = torch.argmax(outputs, dim=1)\n",
        "# acc = accuracy(predicted_labels, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "26_ruDH1eXvI",
        "outputId": "b441b4e9-8dd7-47e6-85eb-550e43dee20b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Accuracy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bcd61b489248>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Accuracy' is not defined"
          ]
        }
      ],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "metric = accuracy()\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "evaluate(model, criterion, metric, dev_data_loader, phase=\"dev\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNhuBvwQJKVW6Qfk9bePSF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}